{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nkw8I8vAFU69"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import time\n",
        "\n",
        "import wandb\n",
        "\n",
        "# Initialize wandb\n",
        "wandb.login(key='208eb9fbdf5d2187fde3a83cdf51d2c458066577')\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Define constants for special tokens\n",
        "SOS_token = 0\n",
        "EOS_token = 1\n",
        "\n",
        "# Language class to handle vocabulary\n",
        "class Language:\n",
        "    def __init__(self, name):\n",
        "        self.name = name\n",
        "        self.word2index = {}\n",
        "        self.word2count = {}\n",
        "        self.index2word = {SOS_token: \"<SOS>\", EOS_token: \"<EOS>\"}\n",
        "        self.n_words = 2  # Count SOS and EOS\n",
        "\n",
        "    def addWord(self, word):\n",
        "        for char in word:\n",
        "            self.addChar(char)\n",
        "\n",
        "    def addChar(self, char):\n",
        "        if char not in self.word2index:\n",
        "            self.word2index[char] = self.n_words\n",
        "            self.word2count[char] = 1\n",
        "            self.index2word[self.n_words] = char\n",
        "            self.n_words += 1\n",
        "        else:\n",
        "            self.word2count[char] += 1\n",
        "\n",
        "# Function to load and preprocess data\n",
        "def load_data(language, data_type):\n",
        "    path = f\"/kaggle/input/akshantar-data/aksharantar_sampled/{language}/{language}_{data_type}.csv\"\n",
        "    df = pd.read_csv(path, header=None)\n",
        "    pairs = df.values.tolist()\n",
        "    return pairs\n",
        "\n",
        "# Function to prepare language objects and data\n",
        "def prepare_data(language):\n",
        "    input_lang = Language('eng')\n",
        "    output_lang = Language(language)\n",
        "    pairs = load_data(language, \"train\")\n",
        "    for pair in pairs:\n",
        "        input_lang.addWord(pair[0])\n",
        "        output_lang.addWord(pair[1])\n",
        "    return input_lang, output_lang, pairs\n",
        "\n",
        "# Function to get cell type for RNN\n",
        "def get_cell(cell_type):\n",
        "    if cell_type == \"LSTM\":\n",
        "        return nn.LSTM\n",
        "    elif cell_type == \"GRU\":\n",
        "        return nn.GRU\n",
        "    elif cell_type == \"RNN\":\n",
        "        return nn.RNN\n",
        "    else:\n",
        "        raise ValueError(\"Invalid cell type\")\n",
        "\n",
        "# Function to get optimizer\n",
        "def get_optimizer(optimizer):\n",
        "    if optimizer == \"Adam\":\n",
        "        return optim.Adam\n",
        "    else:\n",
        "        raise ValueError(\"Invalid optimizer\")\n",
        "\n",
        "# Encoder class\n",
        "class Encoder(nn.Module):\n",
        "    def __init__(self, input_size, embed_size, hidden_size, cell_type, num_layers, dropout):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(input_size, embed_size)\n",
        "        self.rnn = get_cell(cell_type)(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embedded = self.embedding(input).view(1, 1, -1)\n",
        "        output, hidden = self.rnn(embedded, hidden)\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        # Initialize hidden state with zeros\n",
        "        num_layers = self.rnn.num_layers\n",
        "        return torch.zeros(num_layers, 1, self.hidden_size, device=device)\n",
        "\n",
        "# Decoder class\n",
        "class Decoder(nn.Module):\n",
        "    def __init__(self, output_size, embed_size, hidden_size, cell_type, num_layers, dropout):\n",
        "        super(Decoder, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.embedding = nn.Embedding(output_size, embed_size)\n",
        "        self.rnn = get_cell(cell_type)(input_size=embed_size, hidden_size=hidden_size, num_layers=num_layers, dropout=dropout)\n",
        "        self.out = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.LogSoftmax(dim=1)\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        output = self.embedding(input).view(1, 1, -1)\n",
        "        output = F.relu(output)\n",
        "        output, hidden = self.rnn(output, hidden)\n",
        "        output = self.softmax(self.out(output[0]))\n",
        "        return output, hidden\n",
        "\n",
        "    def initHidden(self):\n",
        "        return torch.zeros(self.rnn.num_layers, 1, self.hidden_size, device=device)\n",
        "\n",
        "# Function to convert word to tensor of indices\n",
        "def tensorFromWord(lang, word):\n",
        "    indexes = [lang.word2index[char] for char in word]\n",
        "    indexes.append(EOS_token)\n",
        "    return torch.tensor(indexes, dtype=torch.long, device=device).view(-1, 1)\n",
        "\n",
        "# Function to prepare tensors from pairs\n",
        "def tensorsFromPair(input_lang, output_lang, pair):\n",
        "    input_tensor = tensorFromWord(input_lang, pair[0])\n",
        "    target_tensor = tensorFromWord(output_lang, pair[1])\n",
        "    return input_tensor, target_tensor\n",
        "\n",
        "# Function to train the model\n",
        "def train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion, max_length=50):\n",
        "    encoder_hidden = encoder.initHidden()\n",
        "    encoder_optimizer.zero_grad()\n",
        "    decoder_optimizer.zero_grad()\n",
        "\n",
        "    input_length = input_tensor.size(0)\n",
        "    target_length = target_tensor.size(0)\n",
        "\n",
        "    loss = 0\n",
        "\n",
        "    for ei in range(input_length):\n",
        "        encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "\n",
        "    decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "    decoder_hidden = encoder_hidden\n",
        "\n",
        "    for di in range(target_length):\n",
        "        decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "        topv, topi = decoder_output.topk(1)\n",
        "        decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        loss += criterion(decoder_output, target_tensor[di])\n",
        "        if decoder_input.item() == EOS_token:\n",
        "            break\n",
        "\n",
        "    loss.backward()\n",
        "    encoder_optimizer.step()\n",
        "    decoder_optimizer.step()\n",
        "\n",
        "    return loss.item() / target_length\n",
        "\n",
        "# Function to evaluate the model\n",
        "def evaluate(encoder, decoder, word, input_lang, output_lang, max_length=50):\n",
        "    with torch.no_grad():\n",
        "        input_tensor = tensorFromWord(input_lang, word)\n",
        "        input_length = input_tensor.size()[0]\n",
        "        encoder_hidden = encoder.initHidden()\n",
        "\n",
        "        for ei in range(input_length):\n",
        "            encoder_output, encoder_hidden = encoder(input_tensor[ei], encoder_hidden)\n",
        "\n",
        "        decoder_input = torch.tensor([[SOS_token]], device=device)\n",
        "        decoder_hidden = encoder_hidden\n",
        "\n",
        "        decoded_chars = \"\"\n",
        "\n",
        "        for di in range(max_length):\n",
        "            decoder_output, decoder_hidden = decoder(decoder_input, decoder_hidden)\n",
        "            topv, topi = decoder_output.topk(1)\n",
        "\n",
        "            if topi.item() == EOS_token:\n",
        "                break\n",
        "            else:\n",
        "                decoded_chars += output_lang.index2word[topi.item()]\n",
        "\n",
        "            decoder_input = topi.squeeze().detach()\n",
        "\n",
        "        return decoded_chars\n",
        "\n",
        "# Function to test the model\n",
        "def test(encoder, decoder, language, data_type):\n",
        "    pairs = load_data(language, data_type)\n",
        "    accuracy = np.sum([evaluate(encoder, decoder, pair[0], input_lang, output_lang) == pair[1] for pair in pairs])\n",
        "    return accuracy / len(pairs)\n",
        "\n",
        "# Main training function\n",
        "def train_model(input_lang, output_lang, pairs, config):\n",
        "    # Initialize encoder and decoder\n",
        "    encoder = Encoder(input_lang.n_words, config.embed_size, config.hidden_size, config.cell_type, config.num_layers, config.dropout).to(device)\n",
        "    decoder = Decoder(output_lang.n_words, config.embed_size, config.hidden_size, config.cell_type, config.num_layers, config.dropout).to(device)\n",
        "\n",
        "    # Define optimizer and criterion\n",
        "    encoder_optimizer = get_optimizer(config.optimizer)(encoder.parameters(), lr=config.learning_rate)\n",
        "    decoder_optimizer = get_optimizer(config.optimizer)(decoder.parameters(), lr=config.learning_rate)\n",
        "    criterion = nn.NLLLoss()\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(config.epochs):\n",
        "        total_loss = 0\n",
        "        for pair in pairs:\n",
        "            input_tensor, target_tensor = tensorsFromPair(input_lang, output_lang, pair)\n",
        "            loss = train(input_tensor, target_tensor, encoder, decoder, encoder_optimizer, decoder_optimizer, criterion)\n",
        "            total_loss += loss\n",
        "\n",
        "        # Log training loss\n",
        "        wandb.log({\"epoch\": epoch+1})\n",
        "        wandb.log({\"Training_Loss\": total_loss / len(pairs)})\n",
        "\n",
        "    # Test the model\n",
        "    validation_accuracy = test(encoder, decoder, output_lang.name, \"valid\")\n",
        "    wandb.log({\"validation_accuracy\": validation_accuracy})\n",
        "\n",
        "# Define configuration for hyperparameter sweep\n",
        "sweep_config = {\n",
        "    \"method\": \"bayes\",\n",
        "    \"name\": \"without attention sweep\",\n",
        "    \"metric\": {\"name\": \"validation_accuracy\", \"goal\": \"maximize\"},\n",
        "    \"parameters\": {\n",
        "        \"embed_size\": {\"values\": [16, 32, 64]},\n",
        "        \"hidden_size\": {\"values\": [128, 256, 512]},\n",
        "        \"cell_type\": {\"values\": [\"LSTM\", \"GRU\"]},\n",
        "        \"num_layers\": {\"values\": [1, 2, 3]},\n",
        "        \"dropout\": {\"values\": [0, 0.1, 0.2]},\n",
        "        \"learning_rate\": {\"value\": 0.001},\n",
        "        \"optimizer\": {\"value\": \"Adam\"},\n",
        "        \"epochs\": {\"value\": 5}\n",
        "    },\n",
        "}\n",
        "\n",
        "# Initialize wandb sweep\n",
        "wandb.sweep(sweep_config)\n",
        "\n",
        "# Main function to train models using sweep\n",
        "def train_sweep():\n",
        "    # Initialize wandb run\n",
        "    wandb.init()\n",
        "\n",
        "    # Load data and prepare languages\n",
        "    input_lang, output_lang, pairs = prepare_data('hin')\n",
        "\n",
        "    # Get config from wandb sweep\n",
        "    config = wandb.config\n",
        "\n",
        "    # Train model\n",
        "    train_model(input_lang, output_lang, pairs, config)\n",
        "\n",
        "# Run sweep\n",
        "wandb_id = wandb.sweep(sweep_config, project=\"CS6910_Assignment_3\")\n",
        "wandb.agent(wandb_id, train_sweep, count=50)\n"
      ]
    }
  ]
}