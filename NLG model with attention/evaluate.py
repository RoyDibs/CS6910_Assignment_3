# -*- coding: utf-8 -*-
"""evaluate.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VcaCSuWj_TjKdwuGTLZj9As7YZhGxjvE
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np

from data_convert import tensor_from_word, indexes_from_word, tensors_from_pair

# Set device
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Define constants for special tokens
SOS_token = 0
EOS_token = 1

def evaluate(model, word):
    with torch.no_grad():
        input_tensor = tensor_from_word(model.input_lang, word)
        input_length = input_tensor.size()[0]
        encoder_hidden = model.encoder.init_hidden()
        encoder_cell = model.encoder.init_hidden()
        encoder_outputs = torch.zeros(model.max_length, model.encoder.hidden_size, device=device)
        for ei in range(input_length):
            encoder_output, encoder_hidden, encoder_cell = model.encoder(input_tensor[ei], encoder_hidden, encoder_cell)
            encoder_outputs[ei] += encoder_output[0, 0]
        decoder_input = torch.tensor([[SOS_token]], device=device)
        decoder_hidden, decoder_cell = encoder_hidden, encoder_cell
        decoded_chars = ""
        for di in range(model.max_length):
            decoder_output, decoder_hidden, decoder_cell, _ = model.decoder(decoder_input, decoder_hidden, decoder_cell, encoder_outputs)
            topv, topi = decoder_output.topk(1)
            if topi.item() == EOS_token:
                break
            else:
                decoded_chars += model.output_lang.index2char[topi.item()]
            decoder_input = topi.squeeze().detach()
        return decoded_chars